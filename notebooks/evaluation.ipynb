{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "def check_os():\n",
    "    os = platform.system()\n",
    "\n",
    "    if os == \"Darwin\":\n",
    "        return \"MacOS\"\n",
    "    elif os == \"Linux\":\n",
    "        return \"Linux\"\n",
    "    else:\n",
    "        return \"Unknown OS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "operating_system = check_os()\n",
    "\n",
    "if operating_system == \"MacOS\":\n",
    "    root_path = \"/Users/johnny/Projects/\"\n",
    "elif operating_system == \"Linux\":\n",
    "    root_path = \"/home/johnny/Projects/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(root_path + \"/Master_Thesis_CV/datasets/test_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn for better visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a bar plot for the distribution of \"loitering\" values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=gt_df, x='loitering')\n",
    "\n",
    "plt.title('Distribution of loitering values')\n",
    "plt.xlabel('Value of loitering')\n",
    "plt.ylabel('Unique ID\\'s counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(root_path + \"/Master_Thesis_CV/figures/loitering_distribution.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth and predictions dataframes\n",
    "gt_df = pd.read_csv(root_path + 'Master_Thesis_CV/datasets/test_df.csv')\n",
    "predictions_df = pd.read_csv(root_path + 'Master_Thesis_CV/datasets/predictions_NO_IMP.csv')\n",
    "\n",
    "# Ensure the IDs are aligned for both dataframes\n",
    "gt_df = gt_df.sort_values(by='id').reset_index(drop=True)\n",
    "predictions_df = predictions_df.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "# copy loitering column in predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each method\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "metrics_data = {}\n",
    "methods = [\"rectangle\", \"convex_hull\", \"ellipse\", \"closed_areas\", \"no_motion_short_term\",\"no_motion_long_term\"]\n",
    "\n",
    "\n",
    "gt_values = gt_df['loitering'].values\n",
    "\n",
    "for method in methods:\n",
    "    pred_values = predictions_df[method].values\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(gt_values, pred_values).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "    \n",
    "    metrics_data[method] = {\n",
    "        \"Accuracy\": accuracy_score(gt_values, pred_values),\n",
    "        \"Precision\": precision_score(gt_values, pred_values),\n",
    "        \"Recall\": recall_score(gt_values, pred_values),\n",
    "        \"F1 Score\": f1_score(gt_values, pred_values),\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"ROC AUC\": roc_auc_score(gt_values, pred_values)\n",
    "    }\n",
    "\n",
    "# Convert the metrics data to a dataframe for easier visualization\n",
    "metrics_df = pd.DataFrame(metrics_data).transpose()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rectangle': {'Not loitering': {'precision': 0.8100953053300388,\n",
       "   'recall': 0.7226070528967254,\n",
       "   'f1-score': 0.7638542186719921,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.20986547085201793,\n",
       "   'recall': 0.30310880829015546,\n",
       "   'f1-score': 0.24801271860095386,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.6405775075987842,\n",
       "  'macro avg': {'precision': 0.5099803880910284,\n",
       "   'recall': 0.5128579305934404,\n",
       "   'f1-score': 0.5059334686364729,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.6927251350623002,\n",
       "   'recall': 0.6405775075987842,\n",
       "   'f1-score': 0.6629855160238559,\n",
       "   'support': 3948.0}},\n",
       " 'convex_hull': {'Not loitering': {'precision': 0.8451964216258265,\n",
       "   'recall': 0.6841939546599496,\n",
       "   'f1-score': 0.7562206368540108,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.2716049382716049,\n",
       "   'recall': 0.4844559585492228,\n",
       "   'f1-score': 0.34806886924150765,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.6451367781155015,\n",
       "  'macro avg': {'precision': 0.5584006799487158,\n",
       "   'recall': 0.5843249566045862,\n",
       "   'f1-score': 0.5521447530477592,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.7330351690550415,\n",
       "   'recall': 0.6451367781155015,\n",
       "   'f1-score': 0.676409804889256,\n",
       "   'support': 3948.0}},\n",
       " 'ellipse': {'Not loitering': {'precision': 0.7942336874051593,\n",
       "   'recall': 0.823992443324937,\n",
       "   'f1-score': 0.8088394374903415,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.1439509954058193,\n",
       "   'recall': 0.12176165803108809,\n",
       "   'f1-score': 0.1319298245614035,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.686676798378926,\n",
       "  'macro avg': {'precision': 0.4690923414054893,\n",
       "   'recall': 0.47287705067801256,\n",
       "   'f1-score': 0.47038463102587247,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.6670760789392296,\n",
       "   'recall': 0.686676798378926,\n",
       "   'f1-score': 0.6764751464110255,\n",
       "   'support': 3948.0}},\n",
       " 'closed_areas': {'Not loitering': {'precision': 0.8471264367816091,\n",
       "   'recall': 0.9282115869017632,\n",
       "   'f1-score': 0.8858173076923077,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.5128205128205128,\n",
       "   'recall': 0.31088082901554404,\n",
       "   'f1-score': 0.3870967741935484,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.8074974670719351,\n",
       "  'macro avg': {'precision': 0.679973474801061,\n",
       "   'recall': 0.6195462079586536,\n",
       "   'f1-score': 0.636457040942928,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.7817555722177879,\n",
       "   'recall': 0.8074974670719351,\n",
       "   'f1-score': 0.7882964738875858,\n",
       "   'support': 3948.0}},\n",
       " 'no_motion_short_term': {'Not loitering': {'precision': 0.6913946587537092,\n",
       "   'recall': 0.44017632241813603,\n",
       "   'f1-score': 0.5378991919969218,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.07684319833852545,\n",
       "   'recall': 0.19170984455958548,\n",
       "   'f1-score': 0.10971089696071164,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.39159067882472137,\n",
       "  'macro avg': {'precision': 0.38411892854611734,\n",
       "   'recall': 0.31594308348886074,\n",
       "   'f1-score': 0.32380504447881675,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.571224008439494,\n",
       "   'recall': 0.39159067882472137,\n",
       "   'f1-score': 0.4541703764528604,\n",
       "   'support': 3948.0}},\n",
       " 'no_motion_long_term': {'Not loitering': {'precision': 0.8255850234009361,\n",
       "   'recall': 0.8331234256926953,\n",
       "   'f1-score': 0.8293370944992947,\n",
       "   'support': 3176.0},\n",
       "  'Loitering': {'precision': 0.28667563930013457,\n",
       "   'recall': 0.2759067357512953,\n",
       "   'f1-score': 0.28118811881188116,\n",
       "   'support': 772.0},\n",
       "  'accuracy': 0.7241641337386018,\n",
       "  'macro avg': {'precision': 0.5561303313505354,\n",
       "   'recall': 0.5545150807219953,\n",
       "   'f1-score': 0.5552626066555879,\n",
       "   'support': 3948.0},\n",
       "  'weighted avg': {'precision': 0.720205579498753,\n",
       "   'recall': 0.7241641337386018,\n",
       "   'f1-score': 0.722150921948463,\n",
       "   'support': 3948.0}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gt_values = gt_df['loitering'].values\n",
    "\n",
    "# Methods to be evaluated\n",
    "methods = [\"rectangle\", \"convex_hull\", \"ellipse\", \"closed_areas\", \"no_motion_short_term\", \"no_motion_long_term\"]\n",
    "\n",
    "target_names = ['Not loitering', 'Loitering']\n",
    "\n",
    "# Initialize a dictionary to hold classification reports for each method\n",
    "classification_reports = {}\n",
    "\n",
    "# Evaluate each method\n",
    "for method in methods:\n",
    "    pred_values = predictions_df[method].values\n",
    "    report = classification_report(gt_values, pred_values, output_dict=True, zero_division=0, target_names=target_names)\n",
    "    classification_reports[method] = report\n",
    "\n",
    "classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: rectangle\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |    0.81   | 0.723  |  0.764   |  3176.0 |\n",
      "|   Loitering   |    0.21   | 0.303  |  0.248   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.64\n",
      "========================================\n",
      "Method: convex_hull\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |   0.845   | 0.684  |  0.756   |  3176.0 |\n",
      "|   Loitering   |   0.272   | 0.484  |  0.348   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.65\n",
      "========================================\n",
      "Method: ellipse\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |   0.794   | 0.824  |  0.809   |  3176.0 |\n",
      "|   Loitering   |   0.144   | 0.122  |  0.132   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.69\n",
      "========================================\n",
      "Method: closed_areas\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |   0.847   | 0.928  |  0.886   |  3176.0 |\n",
      "|   Loitering   |   0.513   | 0.311  |  0.387   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.81\n",
      "========================================\n",
      "Method: no_motion_short_term\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |   0.691   |  0.44  |  0.538   |  3176.0 |\n",
      "|   Loitering   |   0.077   | 0.192  |   0.11   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.39\n",
      "========================================\n",
      "Method: no_motion_long_term\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "|     Class     | Precision | Recall | F1-Score | Support |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "| Not loitering |   0.826   | 0.833  |  0.829   |  3176.0 |\n",
      "|   Loitering   |   0.287   | 0.276  |  0.281   |  772.0  |\n",
      "+---------------+-----------+--------+----------+---------+\n",
      "Accuracy: 0.72\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "for method, report in classification_reports.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
    "    \n",
    "    for target_name, metrics in report.items():\n",
    "        if target_name != 'accuracy' and target_name != 'macro avg' and target_name != 'weighted avg':\n",
    "            table.add_row([target_name, \n",
    "                           round(metrics['precision'], 3), \n",
    "                           round(metrics['recall'], 3),\n",
    "                           round(metrics['f1-score'], 3),\n",
    "                           metrics['support']])\n",
    "    \n",
    "    print(table)\n",
    "    print(f\"Accuracy: {round(report['accuracy'], 2)}\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Check if the IDs match between the two dataframes\n",
    "if not all(gt_df['id'] == predictions_df['id']):\n",
    "    raise ValueError(\"The IDs in the ground truth and predictions do not match.\")\n",
    "\n",
    "# Extract the ground truth labels\n",
    "y_true = gt_df['loitering'].values\n",
    "\n",
    "# Majority Voting\n",
    "# For each instance, count the number of times '1' appears across all methods, and decide the final label based on the majority\n",
    "majority_vote_pred = predictions_df.drop('id', axis=1).apply(lambda row: 1 if np.sum(row) > len(row)/2 else 0, axis=1)\n",
    "\n",
    "predictions_df['majority_vote'] = majority_vote_pred\n",
    "\n",
    "# Compute the evaluation metrics for Majority Voting\n",
    "f1_majority = f1_score(y_true, majority_vote_pred)\n",
    "precision_majority = precision_score(y_true, majority_vote_pred)\n",
    "recall_majority = recall_score(y_true, majority_vote_pred)\n",
    "roc_auc_majority = roc_auc_score(y_true, majority_vote_pred)\n",
    "\n",
    "f1_majority, precision_majority, recall_majority, roc_auc_majority\n",
    "\n",
    "print(f\"Majority Voting Precision: {round(precision_majority, 3)}\")\n",
    "print(f\"Majority Voting Recall: {round(recall_majority, 3)}\")\n",
    "print(f\"Majority Voting F1-Score: {round(f1_majority, 3)}\")\n",
    "print(f\"Majority Voting ROC AUC: {round(roc_auc_majority, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the individual F1-scores for each prediction method to use as weights\n",
    "weights = {}\n",
    "for method in predictions_df.columns[1:]:\n",
    "    f1_individual = f1_score(y_true, predictions_df[method])\n",
    "    weights[method] = f1_individual\n",
    "\n",
    "# Compute the weighted sum of predictions for each instance\n",
    "weighted_vote_pred = predictions_df.drop('id', axis=1).apply(lambda row: np.dot(row, list(weights.values())), axis=1)\n",
    "weighted_vote_final_pred = (weighted_vote_pred >= np.mean(list(weights.values()))).astype(int)\n",
    "\n",
    "predictions_df['weighted_vote'] = weighted_vote_final_pred\n",
    "\n",
    "# Compute the evaluation metrics for Weighted Voting\n",
    "f1_weighted = f1_score(y_true, weighted_vote_final_pred)\n",
    "precision_weighted = precision_score(y_true, weighted_vote_final_pred)\n",
    "recall_weighted = recall_score(y_true, weighted_vote_final_pred)\n",
    "roc_auc_weighted = roc_auc_score(y_true, weighted_vote_final_pred)\n",
    "\n",
    "weights, f1_weighted, precision_weighted, recall_weighted, roc_auc_weighted\n",
    "\n",
    "print(f\"Weighted Voting Precision: {round(precision_weighted, 3)}\")\n",
    "print(f\"Weighted Voting Recall: {round(recall_weighted, 3)}\")\n",
    "print(f\"Weighted Voting F1-Score: {round(f1_weighted, 3)}\")\n",
    "print(f\"Weighted Voting ROC AUC: {round(roc_auc_weighted, 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new predictions\n",
    "predictions_df.to_csv(root_path + 'Master_Thesis_CV/datasets/predictions_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing additional classifiers that can be used as meta-models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define base models based on the user's input methods\n",
    "def implement_stacking(X, y, base_models, meta_models):\n",
    "    # Splitting the data into training and test sets (30% held out for testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Dictionary to store the classification reports for different meta-models\n",
    "    classification_reports = {}\n",
    "    \n",
    "    # Loop through each classifier and perform stacking\n",
    "    for name, model in meta_models:\n",
    "        stacking_clf = StackingClassifier(estimators=base_models, final_estimator=model)\n",
    "        stacking_clf.fit(X_train, y_train)\n",
    "        y_pred = stacking_clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        classification_reports[name] = report\n",
    "        \n",
    "    return classification_reports\n",
    "\n",
    "base_models = [(method, RandomForestClassifier(n_estimators=100, max_depth=2, random_state=42)) for method in methods]\n",
    "\n",
    "# List of classifiers to use as meta-models\n",
    "meta_models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier()),\n",
    "    ('Support Vector Classifier', SVC(probability=True)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "# Implement stacking and get classification reports\n",
    "classification_reports = implement_stacking(predictions_df[methods], gt_values, base_models, meta_models)\n",
    "display(classification_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Prepare the feature matrix and target vector\n",
    "X = predictions_df.drop('id', axis=1).values\n",
    "\n",
    "# Initialize the meta-classifier (Logistic Regression)\n",
    "meta_clf = LogisticRegression()\n",
    "\n",
    "# Use cross-validation to get the predicted labels\n",
    "stacking_pred = cross_val_predict(meta_clf, X, y_true, cv=5)\n",
    "\n",
    "# Compute the evaluation metrics for Stacking with cross-validation\n",
    "f1_stacking = f1_score(y_true, stacking_pred)\n",
    "precision_stacking = precision_score(y_true, stacking_pred)\n",
    "recall_stacking = recall_score(y_true, stacking_pred)\n",
    "roc_auc_stacking = roc_auc_score(y_true, stacking_pred)\n",
    "\n",
    "f1_stacking, precision_stacking, recall_stacking, roc_auc_stacking\n",
    "print(f\"Stacking Precision: {round(precision_stacking, 3)}\")\n",
    "print(f\"Stacking Recall: {round(recall_stacking, 3)}\")\n",
    "print(f\"Stacking F1-Score: {round(f1_stacking, 3)}\")\n",
    "print(f\"Stacking ROC AUC: {round(roc_auc_stacking, 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize meta-classifiers\n",
    "meta_clfs = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Dictionary to store the evaluation metrics for each meta-classifier\n",
    "stacking_metrics = {}\n",
    "\n",
    "# Perform stacking with cross-validation for each meta-classifier\n",
    "for name, meta_clf in meta_clfs.items():\n",
    "    stacking_pred = cross_val_predict(meta_clf, X, y_true, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Compute the evaluation metrics\n",
    "    f1 = f1_score(y_true, stacking_pred)\n",
    "    precision = precision_score(y_true, stacking_pred)\n",
    "    recall = recall_score(y_true, stacking_pred)\n",
    "    roc_auc = roc_auc_score(y_true, stacking_pred)\n",
    "    \n",
    "    # Store the metrics\n",
    "    stacking_metrics[name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "stacking_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Dictionary to store the detailed evaluation metrics for each meta-classifier\n",
    "stacking_detailed_metrics = {}\n",
    "\n",
    "# Perform stacking with cross-validation for each meta-classifier\n",
    "for name, meta_clf in meta_clfs.items():\n",
    "    stacking_pred = cross_val_predict(meta_clf, X, y_true, cv=10, n_jobs=-1)\n",
    "    \n",
    "    # Compute the evaluation metrics\n",
    "    f1 = f1_score(y_true, stacking_pred)\n",
    "    precision = precision_score(y_true, stacking_pred)\n",
    "    recall = recall_score(y_true, stacking_pred)\n",
    "    roc_auc = roc_auc_score(y_true, stacking_pred)\n",
    "    \n",
    "    # Class-wise metrics\n",
    "    class_report = classification_report(y_true, stacking_pred, target_names=['Class 0', 'Class 1'], output_dict=True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, stacking_pred).ravel()\n",
    "    \n",
    "    # Store the metrics\n",
    "    stacking_metrics[name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Store the detailed metrics\n",
    "    stacking_detailed_metrics[name] = {\n",
    "        'Classification Report': class_report,\n",
    "        'True Negative': tn,\n",
    "        'False Positive': fp,\n",
    "        'False Negative': fn,\n",
    "        'True Positive': tp,\n",
    "    }\n",
    "\n",
    "# Display summary metrics\n",
    "print(\"Summary Metrics:\")\n",
    "print(stacking_metrics)\n",
    "\n",
    "# Display detailed metrics\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "for name, metrics in stacking_detailed_metrics.items():\n",
    "    print(f\"\\nFor {name}:\")\n",
    "    print(f\"Classification Report: {metrics['Classification Report']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Dictionary to store detailed metrics\n",
    "detailed_metrics = {}\n",
    "\n",
    "# Function to calculate class-wise metrics\n",
    "def get_classwise_metrics(y_true, y_pred):\n",
    "    class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'Classification Report': class_report,\n",
    "        'True Negative': tn,\n",
    "        'False Positive': fp,\n",
    "        'False Negative': fn,\n",
    "        'True Positive': tp\n",
    "    }\n",
    "\n",
    "# For Majority Voting\n",
    "majority_metrics = get_classwise_metrics(y_true, majority_vote_pred)\n",
    "detailed_metrics['Majority Voting'] = majority_metrics\n",
    "\n",
    "# For Weighted Voting\n",
    "weighted_metrics = get_classwise_metrics(y_true, weighted_vote_final_pred)\n",
    "detailed_metrics['Weighted Voting'] = weighted_metrics\n",
    "\n",
    "# Display detailed metrics\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "for name, metrics in detailed_metrics.items():\n",
    "    print(f\"\\nFor {name}:\")\n",
    "    print(f\"Classification Report: {metrics['Classification Report']}\")\n",
    "    print(f\"True Negative: {metrics['True Negative']}\")\n",
    "    print(f\"False Positive: {metrics['False Positive']}\")\n",
    "    print(f\"False Negative: {metrics['False Negative']}\")\n",
    "    print(f\"True Positive: {metrics['True Positive']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
